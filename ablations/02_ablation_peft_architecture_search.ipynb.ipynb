{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "82822fbb",
        "2c7a31bc",
        "50188a72",
        "e8ad280c",
        "cf2a74a7",
        "a0344b47",
        "0e0b33ee",
        "b18c851c",
        "36c37a79"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13734423,
          "sourceType": "datasetVersion",
          "datasetId": 8738676
        },
        {
          "sourceId": 13739688,
          "sourceType": "datasetVersion",
          "datasetId": 8742255
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c3426001-9b08-49f4-a74e-2998e83f419a",
      "cell_type": "markdown",
      "source": [
        "# expt 3 : lora + prompt tuning --- to find best model for our loss"
      ],
      "metadata": {
        "id": "c3426001-9b08-49f4-a74e-2998e83f419a"
      }
    },
    {
      "id": "ae36ad12-0c6f-467d-a74a-07f24d245223",
      "cell_type": "markdown",
      "source": [
        "¬†Priority 1: The \"PEFT Architecture Showdown\" (on CrowS-Pairs)¬†Priority 1: The \"PEFT Architecture Showdown\" (on CrowS-Pairs)\n",
        "We must find out which PEFT architecture works best with your Contrastive PLL loss. <br>\n",
        "\n",
        "Goal: To test if LoRA or Prompt Tuning, when trained with your successful Contrastive PLL loss, can also achieve low intrinsic bias (like your 52.0% score).<br>\n",
        "How:<br>\n",
        "Go to your afml-updated-baseline.ipynb notebook.<br>\n",
        "Keep the data (crows_train, crows_eval) and the trainer (ContrastivePLLTrainer) exactly the same.<br>\n",
        "Run 1 (Control): Run the AdaptedMLM (your custom adapter). Result: You already have this: 52.0% bias.<br>\n",
        "Run 2 (New): Create a new model, LoRA_MLM. This will be BertForMaskedLM with LoRA layers added (using the peft library). Train only the LoRA parameters using your ContrastivePLLTrainer.<br>\n",
        "Run 3 (New): Create a new model, Prompt_MLM. This will be BertForMaskedLM with Prompt Tuning. Train only the prompt parameters using your ContrastivePLLTrainer.<br>\n",
        "Success Metric: You get a table comparing the CrowS-Pairs bias % for all three. The winner is the one with the best (lowest) bias score that doesn't hurt perplexity.<br>"
      ],
      "metadata": {
        "id": "ae36ad12-0c6f-467d-a74a-07f24d245223"
      }
    },
    {
      "id": "bbd97dae-e18d-4eb3-a496-e19cbe4caa37",
      "cell_type": "markdown",
      "source": [
        "Your Next Experiment (The \"PEFT Showdown\") <br>\n",
        "So, we will run the \"Step 9\" experiment, but using your winning loss function. <br>\n",
        "Goal: Find out which PEFT architecture (Adapter vs. LoRA vs. Prompt Tuning) works best with your ContrastivePLLTrainer on the full CrowS-Pairs dataset. <br>\n",
        "Dataset: Use the full crows_train (1206 pairs) and crows_eval (302 pairs) from your successful notebook. <br>\n",
        "Loss Function: Use your ContrastivePLLTrainer for all three models. <br>\n",
        "Models to Test: <br>\n",
        "Model A (Control): AdaptedMLM (Your DebiasAdapter). You already have this result: 52.0% Bias. <br>\n",
        "Model B (New): LoRA_MLM (BERT + LoRA layers). <br>\n",
        "Model C (New): Prompt_MLM (BERT + Soft Prompt). <br>\n",
        "The winner of this showdown (e.g., LoRA_MLM) will be our new champion. That is the model we will use to re-run the BiasBios downstream experiment."
      ],
      "metadata": {
        "id": "bbd97dae-e18d-4eb3-a496-e19cbe4caa37"
      }
    },
    {
      "id": "4d1c8fb3-d8a8-492d-827a-60d4f6d657ae",
      "cell_type": "markdown",
      "source": [
        "Block 1: Setup, Imports & Data Loading"
      ],
      "metadata": {
        "id": "4d1c8fb3-d8a8-492d-827a-60d4f6d657ae"
      }
    },
    {
      "id": "yc5U02KvJ2tg",
      "cell_type": "code",
      "source": [
        "# --- 1. Install & Imports ---\n",
        "!pip install -q -U adapters datasets\n",
        "print(\"adapters installed\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, BertForMaskedLM\n",
        "from adapters import AutoAdapterModel, LoRAConfig, PrefixTuningConfig\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import urllib.request\n",
        "import io\n",
        "\n",
        "# Setup Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "# --- 2. Load CrowS-Pairs Data ---\n",
        "print(\"\\nüì• Loading CrowS-Pairs Dataset...\")\n",
        "url = \"https://raw.githubusercontent.com/nyu-mll/crows-pairs/master/data/crows_pairs_anonymized.csv\"\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    df = pd.read_csv(io.StringIO(response.read().decode('utf-8')))\n",
        "\n",
        "crows_full = []\n",
        "for _, row in df.iterrows():\n",
        "    crows_full.append({\n",
        "        'stereotype': row['sent_more'],\n",
        "        'anti_stereotype': row['sent_less'],\n",
        "        'bias_type': row['bias_type']\n",
        "    })\n",
        "\n",
        "# Split 80/20\n",
        "split_idx = int(len(crows_full) * 0.8)\n",
        "crows_train = crows_full[:split_idx]\n",
        "crows_eval = crows_full[split_idx:]\n",
        "print(f\"‚úÖ Loaded {len(crows_train)} training and {len(crows_eval)} evaluation pairs.\")\n",
        "\n",
        "# --- 3. Load Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# --- 4. Define Contrastive PLL Trainer ---\n",
        "class ContrastivePLLTrainer:\n",
        "    def __init__(self, model, tokenizer, device, learning_rate=1e-4):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        # Optimized for PEFT: Update only trainable params\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            [p for p in model.parameters() if p.requires_grad],\n",
        "            lr=learning_rate\n",
        "        )\n",
        "\n",
        "    def pll_score_with_grad(self, text):\n",
        "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "        if len(tokens) <= 2: return torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        total_pll = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
        "        count = 0\n",
        "\n",
        "        # Create batch of masked inputs for efficiency\n",
        "        input_ids_list = []\n",
        "        target_ids_list = []\n",
        "\n",
        "        for i in range(1, len(tokens) - 1):\n",
        "            masked = tokens.copy()\n",
        "            target = masked[i]\n",
        "            masked[i] = self.tokenizer.mask_token_id\n",
        "            input_ids_list.append(masked)\n",
        "            target_ids_list.append(target)\n",
        "\n",
        "        if not input_ids_list: return torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        input_tensor = torch.tensor(input_ids_list, device=self.device)\n",
        "        target_tensor = torch.tensor(target_ids_list, device=self.device)\n",
        "\n",
        "        outputs = self.model(input_tensor)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Gather correct log-probs\n",
        "        # shape: [batch, seq_len, vocab] -> [batch, vocab] at masked positions\n",
        "        # We need indices 1 to len(tokens)-1 matching the batch items\n",
        "        range_indices = torch.arange(1, len(tokens) - 1, device=self.device)\n",
        "\n",
        "        # Extract logits for the specific masked tokens\n",
        "        # Since input_tensor[k] has mask at index k+1, we gather from that index\n",
        "        target_logits = logits[torch.arange(len(input_ids_list)), range_indices]\n",
        "        log_probs = F.log_softmax(target_logits, dim=-1)\n",
        "\n",
        "        token_plls = log_probs.gather(1, target_tensor.unsqueeze(1)).squeeze()\n",
        "        return token_plls.mean()\n",
        "\n",
        "    def train_epoch(self, pairs):\n",
        "        self.model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Shuffle pairs\n",
        "        np.random.shuffle(pairs)\n",
        "\n",
        "        for pair in tqdm(pairs, desc=\"Training\", leave=False):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            pll_stereo = self.pll_score_with_grad(pair['stereotype'])\n",
        "            pll_anti = self.pll_score_with_grad(pair['anti_stereotype'])\n",
        "\n",
        "            # Contrastive Loss: Minimize squared difference\n",
        "            loss = (pll_stereo - pll_anti) ** 2\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        return np.mean(epoch_losses)\n",
        "\n",
        "# --- 5. Evaluation Helpers ---\n",
        "def compute_pll(model, text):\n",
        "    # Simple PLL for evaluation (no grad)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        score = 0.0\n",
        "        tokens = inputs.input_ids[0]\n",
        "        for i in range(1, len(tokens)-1):\n",
        "            tmp = tokens.clone()\n",
        "            tmp[i] = tokenizer.mask_token_id\n",
        "            out = model(tmp.unsqueeze(0))\n",
        "            score += F.log_softmax(out.logits[0, i], dim=-1)[tokens[i]].item()\n",
        "    return score / (len(tokens)-2) if len(tokens) > 2 else 0.0\n",
        "\n",
        "def evaluate_bias(model, eval_pairs):\n",
        "    model.eval()\n",
        "    stereo_wins = 0\n",
        "    for p in tqdm(eval_pairs, desc=\"Evaluating Bias\"):\n",
        "        s_score = compute_pll(model, p['stereotype'])\n",
        "        a_score = compute_pll(model, p['anti_stereotype'])\n",
        "        if s_score > a_score: stereo_wins += 1\n",
        "    return (stereo_wins / len(eval_pairs)) * 100\n",
        "\n",
        "neutral_sents = [\n",
        "    \"The weather is nice today.\", \"I enjoy reading books.\",\n",
        "    \"The sun rises in the east.\", \"Technology is changing fast.\",\n",
        "    \"She walked to the store.\", \"He cooked dinner for friends.\"\n",
        "]\n",
        "\n",
        "def evaluate_perplexity(model):\n",
        "    model.eval()\n",
        "    total_nll = 0\n",
        "    count = 0\n",
        "    for sent in neutral_sents:\n",
        "        nll = -compute_pll(model, sent)\n",
        "        total_nll += nll\n",
        "        count += 1\n",
        "    return np.exp(total_nll / count)"
      ],
      "metadata": {
        "id": "yc5U02KvJ2tg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9075895e-a5f2-4647-8bf5-9cf3a3f4cf4e",
      "cell_type": "markdown",
      "source": [
        "Block 2: Run LoRA Experiment\n",
        "This builds, trains, and evaluates the LoRA version."
      ],
      "metadata": {
        "id": "9075895e-a5f2-4647-8bf5-9cf3a3f4cf4e"
      }
    },
    {
      "id": "741c960d-6a00-44d5-a951-e4f6213c808b",
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"ü•ä ROUND 1: LoRA Architecture\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# 1. Setup Model\n",
        "lora_model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
        "lora_config = LoRAConfig(r=8, alpha=16)\n",
        "lora_model.add_adapter(\"lora_debias\", config=lora_config)\n",
        "lora_model.train_adapter(\"lora_debias\")\n",
        "lora_model.add_masked_lm_head(\"lora_debias\")\n",
        "lora_model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model Ready. Trainable Params: {sum(p.numel() for p in lora_model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# 2. Train\n",
        "print(\"‚è≥ Training LoRA (5 Epochs)...\")\n",
        "lora_trainer = ContrastivePLLTrainer(lora_model, tokenizer, device, learning_rate=3e-4)\n",
        "for ep in range(5):\n",
        "    loss = lora_trainer.train_epoch(crows_train)\n",
        "    print(f\"   Epoch {ep+1}: Loss = {loss:.4f}\")\n",
        "\n",
        "# 3. Save & Evaluate\n",
        "lora_model.save_adapter(\"/kaggle/working/lora_debias\", \"lora_debias\")\n",
        "lora_bias = evaluate_bias(lora_model, crows_eval)\n",
        "lora_ppl = evaluate_perplexity(lora_model)\n",
        "\n",
        "print(f\"\\nüìä LoRA Results:\")\n",
        "print(f\"   Bias Score: {lora_bias:.2f}% (Target: 50%)\")\n",
        "print(f\"   Perplexity: {lora_ppl:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "741c960d-6a00-44d5-a951-e4f6213c808b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff4e3729-3df0-4585-b250-b32307de785b",
      "cell_type": "markdown",
      "source": [
        "Block 3: Run Prompt Tuning Experiment\n",
        "This builds, trains, and evaluates the Prompt Tuning version."
      ],
      "metadata": {
        "id": "ff4e3729-3df0-4585-b250-b32307de785b"
      }
    },
    {
      "id": "3818409d-5b1d-4336-add2-df70157e7082",
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"ü•ä ROUND 2: Prompt Tuning Architecture\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# 1. Setup Model\n",
        "prompt_model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
        "# prefix_length=20 adds 20 virtual tokens\n",
        "prompt_config = PrefixTuningConfig(flat=False, prefix_length=20)\n",
        "prompt_model.add_adapter(\"prompt_debias\", config=prompt_config)\n",
        "prompt_model.train_adapter(\"prompt_debias\")\n",
        "prompt_model.add_masked_lm_head(\"prompt_debias\")\n",
        "prompt_model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model Ready. Trainable Params: {sum(p.numel() for p in prompt_model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# 2. Train (Note Higher Learning Rate for Prompts)\n",
        "print(\"‚è≥ Training Prompt Tuning (5 Epochs)...\")\n",
        "prompt_trainer = ContrastivePLLTrainer(prompt_model, tokenizer, device, learning_rate=1e-2)\n",
        "for ep in range(5):\n",
        "    loss = prompt_trainer.train_epoch(crows_train)\n",
        "    print(f\"   Epoch {ep+1}: Loss = {loss:.4f}\")\n",
        "\n",
        "# 3. Save & Evaluate\n",
        "prompt_model.save_adapter(\"/kaggle/working/prompt_debias\", \"prompt_debias\")\n",
        "prompt_bias = evaluate_bias(prompt_model, crows_eval)\n",
        "prompt_ppl = evaluate_perplexity(prompt_model)\n",
        "\n",
        "print(f\"\\nüìä Prompt Tuning Results:\")\n",
        "print(f\"   Bias Score: {prompt_bias:.2f}% (Target: 50%)\")\n",
        "print(f\"   Perplexity: {prompt_ppl:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3818409d-5b1d-4336-add2-df70157e7082"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "18943a3a-1364-4c0f-9900-024afcd2eccd",
      "cell_type": "markdown",
      "source": [
        "Block 4: Final Showdown Table"
      ],
      "metadata": {
        "id": "18943a3a-1364-4c0f-9900-024afcd2eccd"
      }
    },
    {
      "id": "a97b8157-fed0-4028-80b8-e22535754d31",
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÜ PEFT ARCHITECTURE SHOWDOWN: FINAL RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Architecture':<20} | {'Bias % (Target 50)':<20} | {'Perplexity':<15}\")\n",
        "print(\"-\" * 60)\n",
        "# Assuming your Custom Adapter score from previous run was around 52%\n",
        "print(f\"{'Custom Adapter':<20} | {'~52.00 (Ref)':<20} | {'~15.5 (Ref)':<15}\")\n",
        "print(f\"{'LoRA':<20} | {lora_bias:<20.2f} | {lora_ppl:<15.2f}\")\n",
        "print(f\"{'Prompt Tuning':<20} | {prompt_bias:<20.2f} | {prompt_ppl:<15.2f}\")\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a97b8157-fed0-4028-80b8-e22535754d31"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5dba450-3d4c-4241-ab82-e8421a78eeb0",
      "cell_type": "markdown",
      "source": [
        "Block 5: Visualization Code"
      ],
      "metadata": {
        "id": "f5dba450-3d4c-4241-ab82-e8421a78eeb0"
      }
    },
    {
      "id": "433c4b33-b2a3-45b5-8f89-8f2515850926",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Plot Training Loss Curves ---\n",
        "def plot_training_comparison(lora_losses, prompt_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    epochs = range(1, len(lora_losses) + 1)\n",
        "\n",
        "    # Plot LoRA\n",
        "    plt.plot(epochs, lora_losses, 'o-', linewidth=2, label='LoRA (Weights)', color='#1f77b4')\n",
        "\n",
        "    # Plot Prompt Tuning\n",
        "    plt.plot(epochs, prompt_losses, 's--', linewidth=2, label='Prompt Tuning (Activations)', color='#ff7f0e')\n",
        "\n",
        "    plt.title('PEFT Training Dynamics: LoRA vs. Prompt Tuning', fontsize=14)\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel('Contrastive PLL Loss', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/kaggle/working/loss_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Run the plotter (assuming you have these lists from the previous steps)\n",
        "if 'lora_losses' in locals() and 'prompt_losses' in locals():\n",
        "    plot_training_comparison(lora_losses, prompt_losses)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Training loss lists not found. Did you run the training blocks?\")\n",
        "\n",
        "# --- 2. Plot The \"Pareto Frontier\" (Bias vs. Utility) ---\n",
        "def plot_tradeoff(results):\n",
        "    \"\"\"\n",
        "    results: dict like {'Model Name': (Bias_Score, Perplexity)}\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Define reference lines\n",
        "    plt.axvline(x=50, color='gray', linestyle='--', alpha=0.5, label='Ideal Neutrality (50%)')\n",
        "\n",
        "    colors = ['#2ca02c', '#1f77b4', '#ff7f0e'] # Green, Blue, Orange\n",
        "    markers = ['*', 'o', 's']\n",
        "\n",
        "    for i, (name, (bias, ppl)) in enumerate(results.items()):\n",
        "        plt.scatter(bias, ppl, s=200, color=colors[i], marker=markers[i], label=name, edgecolors='black')\n",
        "        # Annotate\n",
        "        plt.annotate(f\"{name}\\n({bias:.1f}%, {ppl:.1f})\",\n",
        "                     (bias, ppl),\n",
        "                     xytext=(10, 10), textcoords='offset points',\n",
        "                     fontsize=11)\n",
        "\n",
        "    plt.title('The Fairness-Utility Tradeoff', fontsize=16)\n",
        "    plt.xlabel('Stereotype Preference (Closer to 50% is better)', fontsize=12)\n",
        "    plt.ylabel('Perplexity (Lower is better)', fontsize=12)\n",
        "    plt.xlim(40, 80) # Zoom in on the relevant bias range\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/kaggle/working/tradeoff_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Data (Replace these with your actual variables!)\n",
        "# Baseline numbers usually ~58% bias, ~4.5 perplexity\n",
        "final_results = {\n",
        "    \"Baseline (No Debias)\": (58.3, 15.2),\n",
        "    \"LoRA\": (lora_bias, lora_ppl),\n",
        "    \"Prompt Tuning\": (prompt_bias, prompt_ppl)\n",
        "}\n",
        "\n",
        "plot_tradeoff(final_results)"
      ],
      "metadata": {
        "trusted": true,
        "id": "433c4b33-b2a3-45b5-8f89-8f2515850926"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6c6b7669-7058-43fe-a8e9-d36686f4bff5",
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "    üìå &nbsp; Contrastive PLL reduces intrinsic bias while preserving fluency for adapter-style PEFTs; naive LoRA/prompt tuning collapses LM quality.‚Äù\n",
        "</div>"
      ],
      "metadata": {
        "id": "6c6b7669-7058-43fe-a8e9-d36686f4bff5"
      }
    }
  ]
}